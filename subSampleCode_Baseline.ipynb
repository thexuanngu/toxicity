{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9461d1c-c3d3-45f6-994b-b3399a41d6dc",
   "metadata": {},
   "source": [
    "This is how we sampled for hw5 to go faster. Section 1.1.4 is the equivalent of what we could do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae45ac-0f04-41d8-8119-b19f61647892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.3\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", token=\"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\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# Drop NA values\n",
    "df = df.dropna(subset=['prompt', 'response_a', 'response_b']).reset_index(drop=True)\n",
    "\n",
    "# Analyze token lengths\n",
    "df['prompt_length'] = df['prompt'].apply(lambda x: len(tokenizer.encode(str(x), truncation=False)))\n",
    "df['response_a_length'] = df['response_a'].apply(lambda x: len(tokenizer.encode(str(x), truncation=False)))\n",
    "df['response_b_length'] = df['response_b'].apply(lambda x: len(tokenizer.encode(str(x), truncation=False)))\n",
    "\n",
    "# Filter\n",
    "df_filtered = df[\n",
    "    (df['prompt_length'] < 100) &\n",
    "    (df['response_a_length'] < 400) &\n",
    "    (df['response_b_length'] < 400)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d1ab1-838e-4753-ad51-763a5c6df7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset shape after filtering:\", df_filtered.shape)\n",
    "print(\"\\nLabel distribution after filtering:\")\n",
    "print(df_filtered.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a4d36-fddf-4831-8df2-ff41b0fc8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1.4\n",
    "\n",
    "# 1. Random sample 600 from each class\n",
    "sampled_df = pd.concat([\n",
    "    df_filtered[df_filtered['label'] == 0].sample(n=600, random_state=42),\n",
    "    df_filtered[df_filtered['label'] == 1].sample(n=600, random_state=42)\n",
    "])\n",
    "\n",
    "# 2. Train/Validation split (80/20) — stratify on label\n",
    "train_df, val_df = train_test_split(\n",
    "    sampled_df,\n",
    "    test_size=0.2,\n",
    "    stratify=sampled_df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Add total token length column (prompt + response_a + response_b)\n",
    "val_df['total_tokens'] = (\n",
    "    val_df['prompt_length'] +\n",
    "    val_df['response_a_length'] +\n",
    "    val_df['response_b_length']\n",
    ")\n",
    "\n",
    "# 4. Sort validation set by total_tokens descending\n",
    "val_df = val_df.sort_values(by='total_tokens', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386929c-0aee-4a44-bfe4-8d7596492e0f",
   "metadata": {},
   "source": [
    "**1.1.4 Answer**\n",
    "\n",
    "- Memory management: Long samples take more memory — batching them at the beginning or separately avoids unexpected OOM (out-of-memory) mid-epoch\n",
    "- Faster evaluation: Small batches of similar-size sequences = less padding waste = faster evaluation\n",
    "- Reproducibility: Makes evaluation deterministic and orderly (e.g., longest samples are always first evaluated)\n",
    "- Debugging easier: If errors happen (e.g., OOM), they happen early instead of late randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1101063-8db5-45ee-af0f-de10a115b051",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining set shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation set shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m val_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eaf181-cdde-41f7-a10e-ef729284a2de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
