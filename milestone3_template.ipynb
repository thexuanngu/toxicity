{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7613e4",
   "metadata": {},
   "source": [
    "# 📘 Milestone 3 – CS209B Final Project\n",
    "**Team**: Amar Boparai, Andrew Lobo, Conrad Kaminski, Xiaoxuan Zhang, Xuanthe Nguyen\n",
    "**Title**: Sentiment Analysis and Bias Detection in Toxic Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f0a78",
   "metadata": {},
   "source": [
    "## 🔍 Data Description\n",
    "We use the Jigsaw Unintended Bias in Toxicity Classification dataset. This includes over 2M+ comments labeled across categories like:\n",
    "\n",
    "- `toxicity`, `severe_toxicity`, `insult`, `threat`, `obscene`, `identity_attack`\n",
    "- Demographics: `male`, `female`, `black`, `white`, etc.\n",
    "- Text: `comment_text`\n",
    "\n",
    "We are working with a cleaned and filtered version (`df_cleaned`), sampled to 50% of the original for development speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045e3ac",
   "metadata": {},
   "source": [
    "## 🧾 Summary of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd06ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape:\", df_cleaned.shape)\n",
    "df_cleaned.dtypes\n",
    "df_cleaned.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c649d",
   "metadata": {},
   "source": [
    "### 🔢 Class Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db149dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cleaned[['toxicity', 'severe_toxicity', 'insult', 'threat']].hist(bins=50, figsize=(12, 8))\n",
    "plt.suptitle(\"Distribution of Toxicity Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57535c",
   "metadata": {},
   "source": [
    "## 📊 Data Analysis\n",
    "### 🔍 Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df_cleaned[['toxicity', 'severe_toxicity', 'insult', 'threat', 'identity_attack']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Between Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bffd6",
   "metadata": {},
   "source": [
    "### 🧠 Text Length and Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['text_length'] = df_cleaned['comment_text'].apply(len)\n",
    "sns.scatterplot(data=df_cleaned, x='text_length', y='toxicity', alpha=0.2)\n",
    "plt.title(\"Toxicity vs. Comment Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021f132",
   "metadata": {},
   "source": [
    "## 💡 Meaningful Insights\n",
    "- **High correlation** between `toxicity` and `insult`, `obscene`, and `identity_attack` suggests label redundancy — consider PCA or label compression.\n",
    "- **Comment length** moderately correlates with toxicity — longer comments tend to be more toxic.\n",
    "- **Demographic bias**: Certain identity groups (e.g., `black`, `transgender`) show higher average toxicity scores even when not used in toxic context.\n",
    "\n",
    "→ These insights guide how we treat labels, sampling, and debiasing in modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50063c",
   "metadata": {},
   "source": [
    "## 📈 Clean and Labeled Visualizations\n",
    "(Use the earlier plots and ensure axis labels, titles, legends are present.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f602b",
   "metadata": {},
   "source": [
    "## 📌 Summary of Findings\n",
    "- Label distributions are heavily skewed.\n",
    "- Strong inter-label correlation suggests multi-label prediction task may benefit from shared features or reduced dimensionality.\n",
    "- Potential for unintended bias in identity references (e.g., `female`, `black`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063872fa",
   "metadata": {},
   "source": [
    "## ❓ Clear Research Question\n",
    "> Can we build a sentiment analysis model that **accurately predicts toxicity** in comments while **mitigating bias** across demographic identities?\n",
    "\n",
    "Sub-questions:\n",
    "- How do different model architectures handle overlapping labels?\n",
    "- Can we use debiasing techniques (e.g., adversarial training) to reduce disparity across identity groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aba62a",
   "metadata": {},
   "source": [
    "## 🧪 Baseline Model Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ebe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_cleaned['comment_text'], df_cleaned['toxicity'] > 0.5, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_vec, y_train)\n",
    "preds = lr.predict_proba(X_test_vec)[:, 1]\n",
    "print(\"Baseline AUC:\", roc_auc_score(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
